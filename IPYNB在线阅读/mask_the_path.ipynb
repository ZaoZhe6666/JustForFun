{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as Data\n",
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "from VGG_16_BN_with_FM import VGG16\n",
    "import os\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "from collections import Counter\n",
    "import torchvision.datasets as datasets\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from get_critical_units import get_critical_units\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"5\"\n",
    "\n",
    "def get_sensUnit(data_path, unit_num_per):\n",
    "    '''\n",
    "    not use the parameter unit_num_per in this function\n",
    "    assume that the unit num is same with the unit num which one layer in one path includes\n",
    "    '''\n",
    "    with open(data_path, 'rb') as fr:\n",
    "        paths = pickle.load(fr)\n",
    "\n",
    "    units = {}\n",
    "    sensUnits = {}\n",
    "    sta = {}\n",
    "    for layer in range(13):\n",
    "        units[layer] = []\n",
    "        sensUnits[layer] = []\n",
    "    for k in paths.keys():       \n",
    "        for layer in range(13):\n",
    "            units[layer].extend(paths[k][layer])\n",
    "    for layer in range(13):\n",
    "        unit_num = len(paths[list(paths.keys())[0]][layer])\n",
    "        if layer >= 6:\n",
    "            unit_num = len(paths[list(paths.keys())[0]][layer])\n",
    "        if unit_num < 20:\n",
    "            unit_num = 20\n",
    "        \n",
    "        sta[layer] = []\n",
    "        c = Counter(units[layer])\n",
    "        mc = c.most_common(unit_num)\n",
    "        for a, b in mc:\n",
    "            sensUnits[layer].append(a)\n",
    "            sta[layer].append(b)\n",
    "    print(\"sta\", sta)\n",
    "    print(\"sensUnits:\")\n",
    "    print(sensUnits)\n",
    "    return sensUnits\n",
    "\n",
    "\n",
    "def get_sensUnit2(data_path, unit_num_per=30):\n",
    "\n",
    "    with open(data_path, 'rb') as fr:\n",
    "        paths = pickle.load(fr)\n",
    "\n",
    "    units = {}\n",
    "    sensUnits = {}\n",
    "    sta = {}\n",
    "    for layer in range(13):\n",
    "        units[layer] = []\n",
    "        sensUnits[layer] = []\n",
    "    for k in paths.keys():       \n",
    "        for layer in range(13):\n",
    "            units[layer].extend(paths[k][layer])\n",
    "    for layer in range(13):\n",
    "\n",
    "        unit_num = int(feature_size[layer] * unit_num_per / 100) + 1\n",
    "        sta[layer] = []\n",
    "        c = Counter(units[layer])\n",
    "        mc = c.most_common(unit_num)\n",
    "        for a, b in mc:\n",
    "            sensUnits[layer].append(a)\n",
    "            sta[layer].append(b)\n",
    "    print(\"sta\", sta)\n",
    "    print(\"sensUnits:\", sensUnits)\n",
    "    \n",
    "    return sensUnits\n",
    "\n",
    "feature_size = [64, 64, 128, 128, 256, 256, 256, 512, 512, 512, 512, 512, 512]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mask_VGG16(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_classes):\n",
    "        super(mask_VGG16, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)#64 32 32\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu1 = nn.ReLU(inplace=False)\n",
    "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)#64 32 32\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.relu2 = nn.ReLU(inplace=False)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)#64 16 16\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)#128 16 16\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.relu3 = nn.ReLU(inplace=False)\n",
    "        self.conv4 = nn.Conv2d(128, 128, kernel_size=3, padding=1)#128 16 16\n",
    "        self.bn4 = nn.BatchNorm2d(128)\n",
    "        self.relu4 = nn.ReLU(inplace=False)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)#128 8 8\n",
    "        self.conv5 = nn.Conv2d(128, 256, kernel_size=3, padding=1)#256 8 8\n",
    "        self.bn5 = nn.BatchNorm2d(256)\n",
    "        self.relu5 = nn.ReLU(inplace=False)\n",
    "        self.conv6 = nn.Conv2d(256, 256, kernel_size=3, padding=1)#256 8 8\n",
    "        self.bn6 = nn.BatchNorm2d(256)\n",
    "        self.relu6 = nn.ReLU(inplace=False)\n",
    "        self.conv7 = nn.Conv2d(256, 256, kernel_size=3, padding=1)#256 8 8\n",
    "        self.bn7 = nn.BatchNorm2d(256)\n",
    "        self.relu7 = nn.ReLU(inplace=False)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)#256 4 4\n",
    "        self.conv8 = nn.Conv2d(256, 512, kernel_size=3, padding=1)#512 4 4\n",
    "        self.bn8 = nn.BatchNorm2d(512)\n",
    "        self.relu8 = nn.ReLU(inplace=False)\n",
    "        self.conv9 = nn.Conv2d(512, 512, kernel_size=3, padding=1)#512 4 4\n",
    "        self.bn9 = nn.BatchNorm2d(512)\n",
    "        self.relu9 = nn.ReLU(inplace=False)\n",
    "        self.conv10 = nn.Conv2d(512, 512, kernel_size=3, padding=1)#512 4 4\n",
    "        self.bn10 = nn.BatchNorm2d(512)\n",
    "        self.relu10 = nn.ReLU(inplace=False)\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)#512 2 2\n",
    "        self.conv11 = nn.Conv2d(512, 512, kernel_size=3, padding=1)#512 2 2\n",
    "        self.bn11 = nn.BatchNorm2d(512)\n",
    "        self.relu11 = nn.ReLU(inplace=False)\n",
    "        self.conv12 = nn.Conv2d(512, 512, kernel_size=3, padding=1)#512 2 2\n",
    "        self.bn12 = nn.BatchNorm2d(512)\n",
    "        self.relu12 = nn.ReLU(inplace=False)\n",
    "        self.conv13 = nn.Conv2d(512, 512, kernel_size=3, padding=1)#512 2 2\n",
    "        self.bn13 = nn.BatchNorm2d(512)\n",
    "        self.relu13 = nn.ReLU(inplace=False)\n",
    "        self.pool5 = nn.MaxPool2d(kernel_size=2, stride=2)#512 1 1\n",
    "        self.fc1 = nn.Linear(512, num_classes)\n",
    "        \n",
    "    def forward(self, x, layer):\n",
    "\n",
    "        feat_conv1 = self.conv1(x)\n",
    "        feat_bn1 = self.bn1(feat_conv1)\n",
    "        feat_conv1_relu = self.relu1(feat_bn1)\n",
    "        \n",
    "        if layer >= 1:\n",
    "            feat_conv1_relu[:, units[0], :, :] = clean_feature_dict[\"feat_conv1_relu\"][:, units[0], :, :] \n",
    "\n",
    "        feat_conv2 = self.conv2(feat_conv1_relu)\n",
    "        feat_bn2 = self.bn2(feat_conv2)\n",
    "        feat_conv2_relu = self.relu2(feat_bn2)\n",
    "#         print(units[1])\n",
    "        if layer >= 2:\n",
    "            feat_conv2_relu[:, units[1], :, :] = clean_feature_dict[\"feat_conv2_relu\"][:, units[1], :, :] \n",
    "        feat_pool1 = self.pool1(feat_conv2_relu)\n",
    "\n",
    "        feat_conv3 = self.conv3(feat_pool1)\n",
    "        feat_bn3 = self.bn3(feat_conv3)\n",
    "        feat_conv3_relu = self.relu3(feat_bn3)\n",
    "        if layer >= 3:\n",
    "            feat_conv3_relu[:, units[2], :, :] = clean_feature_dict[\"feat_conv3_relu\"][:, units[2], :, :] \n",
    "        feat_conv4 = self.conv4(feat_conv3_relu)\n",
    "        feat_bn4 = self.bn4(feat_conv4)\n",
    "        feat_conv4_relu = self.relu4(feat_bn4)\n",
    "        if layer >= 4:\n",
    "            feat_conv4_relu[:, units[3], :, :] = clean_feature_dict[\"feat_conv4_relu\"][:, units[3], :, :] \n",
    "        feat_pool2 = self.pool2(feat_conv4_relu)\n",
    "\n",
    "        feat_conv5 = self.conv5(feat_pool2)\n",
    "        feat_bn5 = self.bn5(feat_conv5)\n",
    "        feat_conv5_relu = self.relu5(feat_bn5)\n",
    "        if layer >= 5:\n",
    "            feat_conv5_relu[:, units[4], :, :] = clean_feature_dict[\"feat_conv5_relu\"][:, units[4], :, :] \n",
    "        feat_conv6 = self.conv6(feat_conv5_relu)\n",
    "        feat_bn6 = self.bn6(feat_conv6)\n",
    "        feat_conv6_relu = self.relu6(feat_bn6)\n",
    "        if layer >= 6:\n",
    "            feat_conv6_relu[:, units[5], :, :] = clean_feature_dict[\"feat_conv6_relu\"][:, units[5], :, :] \n",
    "        feat_conv7 = self.conv7(feat_conv6_relu)\n",
    "        feat_bn7 = self.bn7(feat_conv7)\n",
    "        feat_conv7_relu = self.relu7(feat_bn7)\n",
    "        if layer >= 7:\n",
    "            feat_conv7_relu[:, units[6], :, :] = clean_feature_dict[\"feat_conv7_relu\"][:, units[6], :, :] \n",
    "        feat_pool3 = self.pool3(feat_conv7_relu)\n",
    "\n",
    "        feat_conv8 = self.conv8(feat_pool3)\n",
    "        feat_bn8 = self.bn8(feat_conv8)\n",
    "        feat_conv8_relu = self.relu8(feat_bn8)\n",
    "        if layer >= 8:\n",
    "            feat_conv8_relu[:, units[7], :, :] = clean_feature_dict[\"feat_conv8_relu\"][:, units[7], :, :] \n",
    "        feat_conv9 = self.conv9(feat_conv8_relu)\n",
    "        feat_bn9 = self.bn9(feat_conv9)\n",
    "        feat_conv9_relu = self.relu9(feat_bn9)\n",
    "        if layer >= 9:\n",
    "            feat_conv9_relu[:, units[8], :, :] = clean_feature_dict[\"feat_conv9_relu\"][:, units[8], :, :] \n",
    "        feat_conv10 = self.conv10(feat_conv9_relu)\n",
    "        feat_bn10 = self.bn10(feat_conv10)\n",
    "        feat_conv10_relu = self.relu10(feat_bn10)\n",
    "        if layer >= 10:\n",
    "            feat_conv10_relu[:, units[9], :, :] = clean_feature_dict[\"feat_conv10_relu\"][:, units[9], :, :] \n",
    "        feat_pool4 = self.pool4(feat_conv10_relu)\n",
    "\n",
    "        feat_conv11 = self.conv11(feat_pool4)\n",
    "        feat_bn11 = self.bn11(feat_conv11)\n",
    "        feat_conv11_relu = self.relu11(feat_bn11)\n",
    "        if layer >= 11:\n",
    "            feat_conv11_relu[:, units[10], :, :] = clean_feature_dict[\"feat_conv11_relu\"][:, units[10], :, :] \n",
    "        feat_conv12 = self.conv12(feat_conv11_relu)\n",
    "        feat_bn12 = self.bn12(feat_conv12)\n",
    "        feat_conv12_relu = self.relu12(feat_bn12)\n",
    "        if layer >= 12:\n",
    "            feat_conv12_relu[:, units[11], :, :] = clean_feature_dict[\"feat_conv12_relu\"][:, units[11], :, :] \n",
    "        feat_conv13 = self.conv13(feat_conv12_relu)\n",
    "        feat_bn13 = self.bn13(feat_conv13)\n",
    "        feat_conv13_relu = self.relu13(feat_bn13)\n",
    "        if layer >= 13:\n",
    "            feat_conv13_relu[:, units[12], :, :] = clean_feature_dict[\"feat_conv13_relu\"][:, units[12], :, :] \n",
    "        feat_pool5 = self.pool5(feat_conv13_relu)\n",
    "\n",
    "        feat_pool5 = feat_pool5.view(feat_pool5.size(0),-1)\n",
    "\n",
    "        after_dropout = F.dropout(feat_pool5, p=0.3, training=self.training,inplace=False)\n",
    "\n",
    "        feat_fc1 = self.fc1(after_dropout)\n",
    "        return feat_fc1, {'feat_conv1': feat_conv1, 'feat_conv2': feat_conv2, 'feat_conv3': feat_conv3, 'feat_conv4': feat_conv4, 'feat_conv5': feat_conv5, 'feat_conv6': feat_conv6, 'feat_conv7': feat_conv7, 'feat_conv8': feat_conv8, 'feat_conv9' : feat_conv9, 'feat_conv10': feat_conv10, 'feat_conv11': feat_conv11, 'feat_conv12': feat_conv12, 'feat_conv13': feat_conv13, 'feat_conv1_relu': feat_conv1_relu, 'feat_conv2_relu': feat_conv2_relu, 'feat_conv3_relu': feat_conv3_relu, 'feat_conv4_relu': feat_conv4_relu, 'feat_conv5_relu': feat_conv5_relu, 'feat_conv6_relu': feat_conv6_relu, 'feat_conv7_relu': feat_conv7_relu, 'feat_conv8_relu': feat_conv8_relu, 'feat_conv9_relu': feat_conv9_relu, 'feat_conv10_relu': feat_conv10_relu, 'feat_conv11_relu': feat_conv11_relu, 'feat_conv12_relu': feat_conv12_relu, 'feat_conv13_relu': feat_conv13_relu, 'feat_fc1': feat_fc1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VGG16(num_classes=10).cuda()\n",
    "model_path = \"/media/trs1/litl/Vanilla_VGG_16_copy.pkl\"\n",
    "# model_path = \"/media/trs1/TheVirtuoso/Model_Sensitivity/BN_VGG/PAT_VGG_16_finetune.pkl\"\n",
    "checkpoint = torch.load(model_path)\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()\n",
    "\n",
    "mask_model = mask_VGG16(10).cuda()\n",
    "model_path = \"/media/trs1/litl/Vanilla_VGG_16_copy.pkl\"\n",
    "checkpoint = torch.load(model_path)\n",
    "mask_model.load_state_dict(checkpoint)\n",
    "mask_model.eval()\n",
    "\n",
    "transform_test = T.Compose([\n",
    "            T.ToTensor()\n",
    "])\n",
    "\n",
    "dataset = datasets.CIFAR10(\n",
    "        root = \"/media/trs1/yuhang/cifar-10\",\n",
    "        train = False,\n",
    "        transform = transform_test,\n",
    "        download = False\n",
    ")\n",
    "dataloader = Data.DataLoader(dataset=dataset, batch_size=64, shuffle=False, num_workers=2, drop_last=False)\n",
    "\n",
    "\n",
    "def read_data_label(data_path, label_path):\n",
    "    with open(data_path, 'rb') as fr:\n",
    "        test_data = pickle.load(fr)\n",
    "        size = len(test_data)\n",
    "    with open(label_path, 'rb') as fr:\n",
    "        test_label = pickle.load(fr)\n",
    "    return test_data, test_label, size\n",
    "\n",
    "attack = \"MI-FGSM\"\n",
    "test_data_path = \"/media/trs1/TheVirtuoso/Model_Sensitivity/new_adv_examples/white-box-vgg16/Vanilla/\" + attack + \"/test_adv(eps_0.031).pkl\"\n",
    "test_label_path = \"/media/trs1/TheVirtuoso/Model_Sensitivity/new_adv_examples/white-box-vgg16/Vanilla/\" + attack + \"/test_label.pkl\"\n",
    "test_data, test_label, size = read_data_label(test_data_path, test_label_path)\n",
    "adv_dataset = Data.TensorDataset(test_data, test_label)\n",
    "adv_dataloader = Data.DataLoader(dataset=adv_dataset, batch_size=64, shuffle=False, num_workers=2, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sta {0: [8919, 8678, 8579, 8492, 8422, 8300, 8292, 7597, 7555, 7554, 7157, 7099, 6697, 6359, 6247, 6169, 5491, 4687, 4538, 4350], 1: [8545, 8511, 8326, 8226, 8075, 8048, 7993, 7826, 7676, 7246, 7191, 6226, 6157, 5827, 5622, 5209, 5145, 5040, 4760, 4468], 2: [8683, 8100, 7917, 7651, 7340, 7306, 6862, 6792, 6758, 6210, 6113, 5717, 5462, 5261, 5226, 5088, 5028, 4980, 4876, 4856, 4686, 4613, 4585, 4510, 4403, 4316, 3986, 3966, 3932, 3679, 3627, 3560, 3466, 3393, 3119, 2909, 2630, 2478, 2023], 3: [8919, 8919, 8797, 7339, 6511, 6400, 6232, 6070, 6024, 5857, 5818, 5612, 5610, 5553, 5368, 5351, 5260, 5223, 4922, 4629, 4578, 4313, 4248, 4227, 4212, 3724, 3582, 3510, 3400, 3229, 3035, 2997, 2793, 2740, 2728, 2698, 2331, 2162, 2125], 4: [8919, 8919, 8918, 8903, 8901, 8888, 8759, 8683, 8675, 8658, 8592, 8255, 8206, 8193, 8139, 8133, 8071, 7831, 7654, 7642, 7610, 7517, 7450, 7444, 7283, 7185, 7175, 7160, 7126, 7059, 6638, 6602, 6503, 6303, 6265, 6166, 6125, 5924, 5846, 5450, 5358, 5265, 5169, 5136, 4872, 4813, 4613, 4523, 4319, 3945, 3903, 3610, 3518, 3491, 3442, 3426, 3274, 3077, 3072, 2999, 2967, 2945, 2842, 2783, 2758, 2738, 2451, 2185, 2174, 2059, 2056, 2016, 1907, 1857, 1849, 1780, 1730], 5: [8918, 8917, 8911, 8903, 8898, 8854, 8837, 8802, 8790, 8697, 8645, 8616, 8559, 8510, 8502, 8465, 8433, 8428, 8351, 8343, 8331, 8253, 7797, 7742, 7723, 7590, 7581, 7569, 7404, 7357, 7354, 6997, 6660, 6580, 6507, 6433, 6406, 6344, 6021, 5621, 5604, 5424, 5407, 5175, 5134, 5014, 4732, 4543, 4145, 3830, 3829, 3808, 3666, 3417, 3278, 2861, 2827, 2809, 2804, 2794, 2416, 2375, 2264, 2255, 2205, 2181, 2172, 2123, 2100, 2091, 2057, 1992, 1933, 1837, 1819, 1708, 1627], 6: [8919, 8919, 8919, 8903, 8892, 8861, 8836, 8819, 8817, 8798, 8779, 8770, 8742, 8720, 8629, 8586, 8565, 8508, 8477, 8341, 8296, 8267, 8244, 8032, 8000, 7971, 7903, 7833, 7767, 7661, 7594, 7393, 7236, 6995, 6983, 6657, 6467, 6385, 6197, 5995, 5948, 5930, 5762, 5761, 5626, 5602, 5252, 5183, 5025, 5024, 4752, 4595, 4446, 4398, 4248, 4078, 4045, 3831, 3716, 3695, 3546, 3066, 2915, 2746, 2057, 2051, 2029, 2002, 1952, 1255, 1098, 972, 934, 920, 821, 794, 717], 7: [8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8918, 8652, 6835, 4703, 3625, 3504, 3336, 3082, 2578, 2538, 2461, 2339, 2208, 2191, 2177, 2168, 2163, 2140, 1948, 1751, 1745, 1715, 1650, 1611, 1439, 1426, 1414, 1400, 1332, 1308, 1290, 1245, 501, 411, 296, 268, 246, 223, 173, 119, 41, 15, 2, 1, 1, 1], 8: [8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8894, 8858, 8728, 8720, 8695, 8695, 8695, 8695, 8695, 8695, 8695, 8695, 8695, 8652, 8639, 8568, 8470, 8325, 8016, 7881, 7823, 7699, 7656, 7656, 7463, 4908, 4166, 4157, 4151, 3898, 3450, 1508, 1239, 1039, 1039, 1039, 1039, 904, 814, 763, 230, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 199, 199, 46, 46, 40, 25, 25, 25], 9: [8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8914, 8909, 8742, 7291, 7083, 2603, 1044, 9], 10: [8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8916, 8916, 8915, 8906, 8904, 8884, 8868, 8868, 8868, 8867, 8867, 8867, 8867, 8866, 8864, 8862, 7629, 7510, 7344, 6654, 6471, 6077, 5507, 2326, 2293, 1410, 1274, 1116, 1113, 1054, 1034, 979, 956, 416, 257, 241, 240, 240, 166, 57, 56, 56, 55, 52, 52, 52, 52, 52, 51, 51, 51, 34, 23, 17, 10, 3, 1], 11: [8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8910, 7835, 5458, 2216, 1619, 373, 298, 48], 12: [6042, 5729, 5623, 5417, 5369, 5305, 5190, 5123, 5080, 5080, 5013, 4988, 4955, 4945, 4933, 4878, 4813, 4799, 4709, 4708, 4689, 4688, 4671, 4654, 4619, 4594, 4583, 4579, 4563, 4562, 4538, 4461, 4461, 4446, 4440, 4439, 4408, 4353, 4353, 4351, 4336, 4320, 4317, 4315, 4308, 4287, 4239, 4228, 4208, 4198, 4189, 4131, 4112, 4095, 4092, 4081, 4049, 4047, 4036, 4031, 3962, 3958, 3926, 3924, 3905, 3893, 3854, 3850, 3847, 3821, 3798, 3797, 3797, 3789, 3783, 3741, 3739, 3701, 3699, 3693, 3683, 3680, 3666, 3663, 3637, 3633, 3614, 3597, 3585, 3584, 3579, 3570, 3567, 3561, 3544, 3510, 3499, 3497, 3492, 3484, 3447, 3417, 3385, 3358, 3346, 3335, 3335, 3300, 3280, 3264, 3260, 3252, 3249, 3248, 3245, 3236, 3217, 3211, 3200, 3198, 3190, 3185, 3178, 3168, 3160, 3151, 3097, 3094, 3073, 3067, 3037, 3033, 3032, 3019, 3019, 3019, 2996, 2992, 2990, 2975, 2973, 2958, 2944, 2930, 2927, 2918, 2901, 2885, 2885, 2860, 2856, 2840, 2805, 2802]}\n",
      "sensUnits: {0: [3, 21, 45, 32, 2, 0, 34, 4, 46, 35, 16, 58, 26, 24, 25, 54, 30, 22, 15, 62], 1: [41, 58, 10, 7, 45, 50, 40, 51, 17, 0, 55, 43, 48, 49, 28, 26, 37, 29, 46, 30], 2: [51, 18, 90, 120, 92, 17, 26, 32, 40, 13, 89, 72, 111, 109, 23, 74, 21, 105, 124, 122, 123, 10, 83, 47, 79, 4, 59, 61, 78, 113, 50, 16, 119, 19, 126, 98, 117, 75, 104], 3: [6, 18, 49, 81, 74, 60, 103, 87, 32, 58, 110, 29, 79, 93, 105, 65, 78, 59, 26, 36, 10, 66, 42, 23, 9, 56, 97, 118, 109, 88, 4, 15, 91, 123, 121, 125, 35, 94, 54], 4: [198, 35, 241, 125, 92, 126, 16, 216, 124, 36, 78, 130, 135, 31, 235, 74, 98, 236, 66, 65, 56, 21, 129, 175, 69, 6, 166, 82, 213, 242, 190, 95, 54, 144, 46, 253, 44, 221, 63, 72, 174, 33, 121, 186, 94, 178, 87, 239, 119, 146, 37, 205, 17, 150, 41, 187, 195, 136, 148, 184, 75, 43, 142, 232, 246, 247, 32, 203, 80, 251, 194, 67, 0, 217, 248, 77, 111], 5: [163, 231, 190, 155, 138, 236, 146, 100, 74, 250, 36, 80, 187, 240, 134, 232, 19, 23, 110, 172, 64, 20, 216, 59, 27, 16, 184, 254, 15, 206, 46, 212, 90, 40, 213, 71, 33, 218, 183, 211, 38, 9, 44, 248, 182, 173, 228, 241, 129, 196, 244, 204, 105, 93, 112, 26, 97, 111, 76, 126, 31, 224, 178, 202, 185, 94, 245, 10, 81, 41, 144, 189, 179, 156, 7, 73, 84], 6: [57, 214, 41, 16, 176, 135, 79, 118, 9, 161, 192, 143, 19, 170, 131, 175, 10, 0, 128, 134, 72, 93, 36, 149, 43, 195, 107, 223, 247, 105, 114, 2, 35, 233, 110, 78, 255, 206, 27, 60, 169, 234, 59, 249, 80, 165, 111, 237, 154, 26, 148, 53, 7, 76, 25, 96, 69, 127, 66, 32, 164, 182, 217, 37, 152, 38, 168, 28, 183, 226, 33, 97, 44, 63, 244, 173, 34], 7: [463, 39, 98, 80, 140, 282, 267, 289, 395, 105, 441, 356, 442, 479, 250, 33, 346, 313, 383, 257, 240, 251, 403, 19, 194, 249, 95, 51, 300, 466, 163, 86, 40, 113, 126, 219, 333, 287, 224, 77, 49, 376, 398, 209, 31, 402, 110, 406, 68, 193, 495, 255, 302, 381, 334, 8, 458, 6, 59, 420, 26, 469, 132, 393, 4, 435, 326, 18, 369, 340, 448, 54, 286, 457, 183, 157, 210, 418, 121, 227, 417, 244, 261, 65, 187, 236, 78, 42, 37, 208, 84, 47, 415, 400, 89, 158, 159, 201, 145, 144, 165, 147, 156, 155, 149, 148, 200, 171, 170, 226, 168, 166, 202, 199, 225, 146, 203, 164, 162, 167, 223, 160, 188, 161, 198, 169, 186, 150, 207, 222, 172, 151, 184, 216, 229, 152, 213, 206], 8: [60, 26, 212, 496, 492, 231, 154, 94, 324, 223, 33, 85, 71, 444, 148, 505, 410, 224, 10, 273, 157, 24, 330, 250, 421, 232, 399, 237, 290, 213, 169, 379, 128, 400, 326, 209, 186, 190, 299, 36, 366, 165, 204, 289, 42, 170, 241, 375, 371, 285, 335, 244, 101, 256, 300, 388, 317, 429, 220, 126, 465, 178, 450, 196, 427, 336, 151, 44, 468, 426, 478, 152, 158, 153, 160, 155, 156, 161, 139, 138, 140, 235, 227, 137, 226, 136, 159, 162, 142, 228, 359, 143, 141, 363, 233, 229, 163, 230, 145, 149, 146, 147, 144, 164, 210, 150, 234, 222, 225, 221, 219, 218, 135, 199, 200, 20, 28, 21, 168, 27, 174, 181, 179, 185, 184, 183, 182, 19, 180, 171, 177, 176, 175, 173, 172, 17, 167, 198, 217, 211, 187, 29, 166], 9: [265, 52, 490, 247, 342, 303, 106, 377, 259, 440, 198, 498, 479, 436, 27, 217, 193, 359, 343, 464, 386, 283, 109, 507, 459, 118, 33, 136, 51, 17, 140, 380, 285, 30, 233, 307, 238, 32, 124, 458, 108, 413, 366, 135, 74, 338, 474, 488, 94, 44, 485, 442, 329, 453, 137, 180, 309, 492, 258, 48, 308, 294, 128, 445, 480, 96, 155, 147, 154, 153, 146, 152, 163, 157, 161, 160, 145, 156, 143, 148, 149, 139, 168, 138, 172, 171, 144, 170, 151, 169, 162, 167, 166, 165, 164, 150, 158, 141, 279, 142, 411, 423, 173, 159, 174], 10: [184, 428, 202, 264, 312, 66, 138, 364, 504, 257, 133, 272, 232, 119, 124, 381, 342, 161, 359, 127, 165, 224, 308, 25, 469, 448, 293, 336, 349, 7, 319, 418, 234, 473, 340, 211, 146, 356, 305, 317, 40, 32, 63, 1, 299, 240, 352, 404, 360, 177, 170, 179, 92, 253, 250, 323, 368, 116, 175, 159, 160, 154, 155, 156, 157, 158, 152, 150, 153, 151, 143, 141, 142, 139, 149, 144, 147, 148, 140, 162, 145, 235, 163, 164, 236, 231, 230, 229, 227, 226, 233, 228, 225, 237, 407, 166, 223, 204, 183, 206, 192, 205, 207, 222, 167, 197, 194, 221, 201, 196, 168, 209, 203, 210, 198, 195, 208, 220, 169, 215, 171, 214, 173, 217, 218, 172, 213, 216, 212, 176, 219, 200, 199, 193, 191, 238], 11: [400, 476, 287, 281, 181, 14, 440, 437, 411, 21, 69, 80, 317, 214, 306, 387, 430, 212, 379, 457, 35, 347, 483, 498, 40, 27, 276, 144, 141, 257, 424, 16, 48, 471, 31, 419, 246, 469, 435, 147, 322, 382, 466, 356, 458, 272, 240, 468, 138, 72, 341, 451, 494, 114, 398, 90, 313, 285, 477, 107, 5, 0, 267, 133, 456, 45, 71, 511, 474, 499, 168, 359, 129, 331, 126, 293, 218, 343, 110, 434, 70, 26, 386, 350, 335, 93, 448, 464, 491, 510, 30, 332, 413, 432, 325, 289, 416, 17, 4, 153, 393, 368, 369, 364, 488, 509, 154], 12: [286, 79, 164, 203, 131, 480, 25, 394, 209, 344, 127, 392, 34, 485, 362, 371, 473, 229, 126, 179, 273, 410, 125, 138, 175, 383, 350, 419, 85, 454, 257, 145, 67, 355, 364, 190, 504, 5, 152, 460, 327, 493, 478, 309, 33, 263, 291, 361, 340, 287, 490, 487, 124, 59, 269, 421, 61, 106, 45, 221, 440, 409, 445, 411, 307, 368, 385, 26, 468, 348, 353, 2, 274, 442, 317, 154, 278, 483, 84, 161, 509, 465, 502, 191, 417, 462, 261, 444, 187, 0, 201, 377, 219, 101, 407, 64, 208, 477, 252, 283, 107, 60, 237, 225, 351, 134, 188, 251, 123, 217, 58, 86, 155, 374, 16, 7, 335, 183, 471, 479, 140, 284, 95, 272, 118, 294, 220, 147, 376, 162, 438, 130, 189, 46, 121, 108, 91, 8, 254, 369, 35, 320, 492, 267, 464, 119, 110, 506, 182, 434, 256, 437, 132, 326]}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: [3, 21, 45, 32, 2, 0, 34, 4, 46, 35, 16, 58, 26, 24, 25, 54, 30, 22, 15, 62], 1: [41, 58, 10, 7, 45, 50, 40, 51, 17, 0, 55, 43, 48, 49, 28, 26, 37, 29, 46, 30], 2: [51, 18, 90, 120, 92, 17, 26, 32, 40, 13, 89, 72, 111, 109, 23, 74, 21, 105, 124, 122, 123, 10, 83, 47, 79, 4, 59, 61, 78, 113, 50, 16, 119, 19, 126, 98, 117, 75, 104], 3: [6, 18, 49, 81, 74, 60, 103, 87, 32, 58, 110, 29, 79, 93, 105, 65, 78, 59, 26, 36, 10, 66, 42, 23, 9, 56, 97, 118, 109, 88, 4, 15, 91, 123, 121, 125, 35, 94, 54], 4: [198, 35, 241, 125, 92, 126, 16, 216, 124, 36, 78, 130, 135, 31, 235, 74, 98, 236, 66, 65, 56, 21, 129, 175, 69, 6, 166, 82, 213, 242, 190, 95, 54, 144, 46, 253, 44, 221, 63, 72, 174, 33, 121, 186, 94, 178, 87, 239, 119, 146, 37, 205, 17, 150, 41, 187, 195, 136, 148, 184, 75, 43, 142, 232, 246, 247, 32, 203, 80, 251, 194, 67, 0, 217, 248, 77, 111], 5: [163, 231, 190, 155, 138, 236, 146, 100, 74, 250, 36, 80, 187, 240, 134, 232, 19, 23, 110, 172, 64, 20, 216, 59, 27, 16, 184, 254, 15, 206, 46, 212, 90, 40, 213, 71, 33, 218, 183, 211, 38, 9, 44, 248, 182, 173, 228, 241, 129, 196, 244, 204, 105, 93, 112, 26, 97, 111, 76, 126, 31, 224, 178, 202, 185, 94, 245, 10, 81, 41, 144, 189, 179, 156, 7, 73, 84], 6: [57, 214, 41, 16, 176, 135, 79, 118, 9, 161, 192, 143, 19, 170, 131, 175, 10, 0, 128, 134, 72, 93, 36, 149, 43, 195, 107, 223, 247, 105, 114, 2, 35, 233, 110, 78, 255, 206, 27, 60, 169, 234, 59, 249, 80, 165, 111, 237, 154, 26, 148, 53, 7, 76, 25, 96, 69, 127, 66, 32, 164, 182, 217, 37, 152, 38, 168, 28, 183, 226, 33, 97, 44, 63, 244, 173, 34], 7: [463, 39, 98, 80, 140, 282, 267, 289, 395, 105, 441, 356, 442, 479, 250, 33, 346, 313, 383, 257, 240, 251, 403, 19, 194, 249, 95, 51, 300, 466, 163, 86, 40, 113, 126, 219, 333, 287, 224, 77, 49, 376, 398, 209, 31, 402, 110, 406, 68, 193, 495, 255, 302, 381, 334, 8, 458, 6, 59, 420, 26, 469, 132, 393, 4, 435, 326, 18, 369, 340, 448, 54, 286, 457, 183, 157, 210, 418, 121, 227, 417, 244, 261, 65, 187, 236, 78, 42, 37, 208, 84, 47, 415, 400, 89, 158, 159, 201, 145, 144, 165, 147, 156, 155, 149, 148, 200, 171, 170, 226, 168, 166, 202, 199, 225, 146, 203, 164, 162, 167, 223, 160, 188, 161, 198, 169, 186, 150, 207, 222, 172, 151, 184, 216, 229, 152, 213, 206], 8: [60, 26, 212, 496, 492, 231, 154, 94, 324, 223, 33, 85, 71, 444, 148, 505, 410, 224, 10, 273, 157, 24, 330, 250, 421, 232, 399, 237, 290, 213, 169, 379, 128, 400, 326, 209, 186, 190, 299, 36, 366, 165, 204, 289, 42, 170, 241, 375, 371, 285, 335, 244, 101, 256, 300, 388, 317, 429, 220, 126, 465, 178, 450, 196, 427, 336, 151, 44, 468, 426, 478, 152, 158, 153, 160, 155, 156, 161, 139, 138, 140, 235, 227, 137, 226, 136, 159, 162, 142, 228, 359, 143, 141, 363, 233, 229, 163, 230, 145, 149, 146, 147, 144, 164, 210, 150, 234, 222, 225, 221, 219, 218, 135, 199, 200, 20, 28, 21, 168, 27, 174, 181, 179, 185, 184, 183, 182, 19, 180, 171, 177, 176, 175, 173, 172, 17, 167, 198, 217, 211, 187, 29, 166], 9: [265, 52, 490, 247, 342, 303, 106, 377, 259, 440, 198, 498, 479, 436, 27, 217, 193, 359, 343, 464, 386, 283, 109, 507, 459, 118, 33, 136, 51, 17, 140, 380, 285, 30, 233, 307, 238, 32, 124, 458, 108, 413, 366, 135, 74, 338, 474, 488, 94, 44, 485, 442, 329, 453, 137, 180, 309, 492, 258, 48, 308, 294, 128, 445, 480, 96, 155, 147, 154, 153, 146, 152, 163, 157, 161, 160, 145, 156, 143, 148, 149, 139, 168, 138, 172, 171, 144, 170, 151, 169, 162, 167, 166, 165, 164, 150, 158, 141, 279, 142, 411, 423, 173, 159, 174], 10: [184, 428, 202, 264, 312, 66, 138, 364, 504, 257, 133, 272, 232, 119, 124, 381, 342, 161, 359, 127, 165, 224, 308, 25, 469, 448, 293, 336, 349, 7, 319, 418, 234, 473, 340, 211, 146, 356, 305, 317, 40, 32, 63, 1, 299, 240, 352, 404, 360, 177, 170, 179, 92, 253, 250, 323, 368, 116, 175, 159, 160, 154, 155, 156, 157, 158, 152, 150, 153, 151, 143, 141, 142, 139, 149, 144, 147, 148, 140, 162, 145, 235, 163, 164, 236, 231, 230, 229, 227, 226, 233, 228, 225, 237, 407, 166, 223, 204, 183, 206, 192, 205, 207, 222, 167, 197, 194, 221, 201, 196, 168, 209, 203, 210, 198, 195, 208, 220, 169, 215, 171, 214, 173, 217, 218, 172, 213, 216, 212, 176, 219, 200, 199, 193, 191, 238], 11: [400, 476, 287, 281, 181, 14, 440, 437, 411, 21, 69, 80, 317, 214, 306, 387, 430, 212, 379, 457, 35, 347, 483, 498, 40, 27, 276, 144, 141, 257, 424, 16, 48, 471, 31, 419, 246, 469, 435, 147, 322, 382, 466, 356, 458, 272, 240, 468, 138, 72, 341, 451, 494, 114, 398, 90, 313, 285, 477, 107, 5, 0, 267, 133, 456, 45, 71, 511, 474, 499, 168, 359, 129, 331, 126, 293, 218, 343, 110, 434, 70, 26, 386, 350, 335, 93, 448, 464, 491, 510, 30, 332, 413, 432, 325, 289, 416, 17, 4, 153, 393, 368, 369, 364, 488, 509, 154], 12: [286, 79, 164, 203, 131, 480, 25, 394, 209, 344, 127, 392, 34, 485, 362, 371, 473, 229, 126, 179, 273, 410, 125, 138, 175, 383, 350, 419, 85, 454, 257, 145, 67, 355, 364, 190, 504, 5, 152, 460, 327, 493, 478, 309, 33, 263, 291, 361, 340, 287, 490, 487, 124, 59, 269, 421, 61, 106, 45, 221, 440, 409, 445, 411, 307, 368, 385, 26, 468, 348, 353, 2, 274, 442, 317, 154, 278, 483, 84, 161, 509, 465, 502, 191, 417, 462, 261, 444, 187, 0, 201, 377, 219, 101, 407, 64, 208, 477, 252, 283, 107, 60, 237, 225, 351, 134, 188, 251, 123, 217, 58, 86, 155, 374, 16, 7, 335, 183, 471, 479, 140, 284, 95, 272, 118, 294, 220, 147, 376, 162, 438, 130, 189, 46, 121, 108, 91, 8, 254, 369, 35, 320, 492, 267, 464, 119, 110, 506, 182, 434, 256, 437, 132, 326]}\n",
      "step: 0\n",
      "step: 1\n",
      "step: 2\n",
      "step: 3\n",
      "step: 4\n",
      "step: 5\n",
      "step: 6\n",
      "step: 7\n",
      "step: 8\n",
      "step: 9\n",
      "step: 10\n",
      "step: 11\n",
      "step: 12\n",
      "step: 13\n",
      "step: 14\n",
      "step: 15\n",
      "step: 16\n",
      "step: 17\n",
      "step: 18\n",
      "step: 19\n",
      "step: 20\n",
      "step: 21\n",
      "step: 22\n",
      "step: 23\n",
      "step: 24\n",
      "step: 25\n",
      "step: 26\n",
      "step: 27\n",
      "step: 28\n",
      "step: 29\n",
      "step: 30\n",
      "step: 31\n",
      "step: 32\n",
      "step: 33\n",
      "step: 34\n",
      "step: 35\n",
      "step: 36\n",
      "step: 37\n",
      "step: 38\n",
      "step: 39\n",
      "step: 40\n",
      "step: 41\n",
      "step: 42\n",
      "step: 43\n",
      "step: 44\n",
      "step: 45\n",
      "step: 46\n",
      "step: 47\n",
      "step: 48\n",
      "step: 49\n",
      "step: 50\n",
      "step: 51\n",
      "step: 52\n",
      "step: 53\n",
      "step: 54\n",
      "step: 55\n",
      "step: 56\n",
      "step: 57\n",
      "step: 58\n",
      "step: 59\n",
      "step: 60\n",
      "step: 61\n",
      "step: 62\n",
      "step: 63\n",
      "step: 64\n",
      "step: 65\n",
      "step: 66\n",
      "step: 67\n",
      "step: 68\n",
      "step: 69\n",
      "step: 70\n",
      "step: 71\n",
      "step: 72\n",
      "step: 73\n",
      "step: 74\n",
      "step: 75\n",
      "step: 76\n",
      "step: 77\n",
      "step: 78\n",
      "step: 79\n",
      "step: 80\n",
      "step: 81\n",
      "step: 82\n",
      "step: 83\n",
      "step: 84\n",
      "step: 85\n",
      "step: 86\n",
      "step: 87\n",
      "step: 88\n",
      "step: 89\n",
      "step: 90\n",
      "step: 91\n",
      "step: 92\n",
      "step: 93\n",
      "step: 94\n",
      "step: 95\n",
      "step: 96\n",
      "step: 97\n",
      "step: 98\n",
      "step: 99\n",
      "step: 100\n",
      "step: 101\n",
      "step: 102\n",
      "step: 103\n",
      "step: 104\n",
      "step: 105\n",
      "step: 106\n",
      "step: 107\n",
      "step: 108\n",
      "step: 109\n",
      "step: 110\n",
      "step: 111\n",
      "step: 112\n",
      "step: 113\n",
      "step: 114\n",
      "step: 115\n",
      "step: 116\n",
      "step: 117\n",
      "step: 118\n",
      "step: 119\n",
      "step: 120\n",
      "step: 121\n",
      "step: 122\n",
      "step: 123\n",
      "step: 124\n",
      "step: 125\n",
      "step: 126\n",
      "step: 127\n",
      "step: 128\n",
      "step: 129\n",
      "step: 130\n",
      "step: 131\n",
      "step: 132\n",
      "step: 133\n",
      "step: 134\n",
      "step: 135\n",
      "step: 136\n",
      "step: 137\n",
      "step: 138\n",
      "step: 139\n",
      "step: 140\n",
      "step: 141\n",
      "step: 142\n",
      "step: 143\n",
      "step: 144\n",
      "step: 145\n",
      "step: 146\n",
      "step: 147\n",
      "step: 148\n",
      "step: 149\n",
      "step: 150\n",
      "step: 151\n",
      "step: 152\n",
      "step: 153\n",
      "step: 154\n",
      "step: 155\n",
      "step: 156\n",
      "[7249, 8853, 8907, 8920]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "start_index = 0\n",
    "\n",
    "for step, (x, y) in enumerate(dataloader): \n",
    "    x = x.cuda()\n",
    "    target, clean_feature_dict = model(x)  \n",
    "    break\n",
    "    \n",
    "unit_nums = [30]  \n",
    "results = []\n",
    "for unit_num in unit_nums:\n",
    "\n",
    "    # root = \"/media/trs1/litl/Vanilla_key_paths_loss_lossOriented_abs/\"\n",
    "    root = \"/media/trs1/litl/path_cifar/Vanilla_key_paths_loss_layer_abs/\"\n",
    "    attack = \"clean\"\n",
    "    data_path = root + \"key_paths_\" + attack + \"/key_paths_unit_per20.pkl\"\n",
    "    units_clean = get_sensUnit2(data_path, unit_num)\n",
    "    \n",
    "    mean_features_specialUnits = {}\n",
    "    mean_features_regularUnits = {}\n",
    "    num_special = {}\n",
    "    num_regular = {}\n",
    "    for k in units_clean.keys():\n",
    "        conv_name = \"feat_conv{}_relu\".format(k+1)\n",
    "        mean_features_specialUnits[conv_name] = 0\n",
    "        mean_features_regularUnits[conv_name] = 0\n",
    "        num_special[conv_name] = 0\n",
    "        num_regular[conv_name] = 0\n",
    "    # c_links_clean = get_criticalLinks(data_path, link_num)\n",
    "\n",
    "\n",
    "#     random_units = {}\n",
    "#     for i in units_clean:\n",
    "#         layer = \"feat_conv\" + str(i+1) + \"_relu\"\n",
    "#         random_units[i] = []\n",
    "#         if unit_num < 50:\n",
    "#             while len(random_units[i]) < len(units_clean[i]):\n",
    "#                 r = random.randint(0, clean_feature_dict[layer].size(1)-1)\n",
    "#                 if not r in units_clean[i] and not r in random_units[i]:\n",
    "#                     random_units[i].append(r)\n",
    "#         else:\n",
    "#             for r in range(clean_feature_dict[layer].size(1)-1):\n",
    "#                 if not r in units_clean[i]:\n",
    "#                     random_units[i].append(r)\n",
    "#     units = random_units\n",
    "    units = units_clean\n",
    "\n",
    "    # print(random_units)\n",
    "    print(units_clean)\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        sample_num = 0\n",
    "        adv_right0 = 0\n",
    "        adv_right1 = 0\n",
    "        adv_right3 = 0\n",
    "        adv_right5 = 0\n",
    "        adv_right7 = 0\n",
    "        clean_right = 0\n",
    "        for step, (x, y) in enumerate(dataloader):    \n",
    "            print(\"step:\", step)\n",
    "            batch_size = x.size(0)\n",
    "            adv_x = adv_dataset[start_index: start_index+batch_size][0]\n",
    "            start_index += x.size(0)\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "            adv_x = adv_x.cuda()\n",
    "\n",
    "            target, clean_feature_dict = model(x)\n",
    "            adv_target0, adv_feature_dict0 = mask_model(adv_x, 0)\n",
    "            adv_target1, adv_feature_dict1 = mask_model(adv_x, 1)\n",
    "            adv_target3, adv_feature_dict3 = mask_model(adv_x, 3)\n",
    "            adv_target5, adv_feature_dict5 = mask_model(adv_x, 5)\n",
    "            adv_target7, adv_feature_dict7 = mask_model(adv_x, 7)\n",
    "\n",
    "            target = torch.max(target.data, 1)[1].data\n",
    "            adv_target0 = torch.max(adv_target0.data, 1)[1].data\n",
    "            adv_target1 = torch.max(adv_target1.data, 1)[1].data\n",
    "            adv_target3 = torch.max(adv_target3.data, 1)[1].data\n",
    "            adv_target5 = torch.max(adv_target5.data, 1)[1].data\n",
    "            adv_target7 = torch.max(adv_target7.data, 1)[1].data\n",
    "\n",
    "            clean_right += (target == y).sum()\n",
    "            adv_right0 += (adv_target0 == y).sum()\n",
    "            adv_right1 += (adv_target1 == y).sum() \n",
    "            adv_right3 += (adv_target3 == y).sum() \n",
    "            adv_right5 += (adv_target5 == y).sum() \n",
    "            adv_right7 += (adv_target7 == y).sum() \n",
    "\n",
    "    print([adv_right0.item(), adv_right1.item(), adv_right3.item(), adv_right5.item(), adv_right7.item()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 高斯噪声"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sta {0: [8919, 8678, 8579, 8492, 8422, 8300, 8292, 7597, 7555, 7554, 7157, 7099, 6697], 1: [8545, 8511, 8326, 8226, 8075, 8048, 7993, 7826, 7676, 7246, 7191, 6226, 6157], 2: [8683, 8100, 7917, 7651, 7340, 7306, 6862, 6792, 6758, 6210, 6113, 5717, 5462, 5261, 5226, 5088, 5028, 4980, 4876, 4856, 4686, 4613, 4585, 4510, 4403, 4316], 3: [8919, 8919, 8797, 7339, 6511, 6400, 6232, 6070, 6024, 5857, 5818, 5612, 5610, 5553, 5368, 5351, 5260, 5223, 4922, 4629, 4578, 4313, 4248, 4227, 4212, 3724], 4: [8919, 8919, 8918, 8903, 8901, 8888, 8759, 8683, 8675, 8658, 8592, 8255, 8206, 8193, 8139, 8133, 8071, 7831, 7654, 7642, 7610, 7517, 7450, 7444, 7283, 7185, 7175, 7160, 7126, 7059, 6638, 6602, 6503, 6303, 6265, 6166, 6125, 5924, 5846, 5450, 5358, 5265, 5169, 5136, 4872, 4813, 4613, 4523, 4319, 3945, 3903, 3610], 5: [8918, 8917, 8911, 8903, 8898, 8854, 8837, 8802, 8790, 8697, 8645, 8616, 8559, 8510, 8502, 8465, 8433, 8428, 8351, 8343, 8331, 8253, 7797, 7742, 7723, 7590, 7581, 7569, 7404, 7357, 7354, 6997, 6660, 6580, 6507, 6433, 6406, 6344, 6021, 5621, 5604, 5424, 5407, 5175, 5134, 5014, 4732, 4543, 4145, 3830, 3829, 3808], 6: [8919, 8919, 8919, 8903, 8892, 8861, 8836, 8819, 8817, 8798, 8779, 8770, 8742, 8720, 8629, 8586, 8565, 8508, 8477, 8341, 8296, 8267, 8244, 8032, 8000, 7971, 7903, 7833, 7767, 7661, 7594, 7393, 7236, 6995, 6983, 6657, 6467, 6385, 6197, 5995, 5948, 5930, 5762, 5761, 5626, 5602, 5252, 5183, 5025, 5024, 4752, 4595], 7: [8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8918, 8652, 6835, 4703, 3625, 3504, 3336, 3082, 2578, 2538, 2461], 8: [8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8894, 8858, 8728, 8720, 8695, 8695, 8695, 8695, 8695, 8695, 8695, 8695, 8695, 8652, 8639, 8568, 8470, 8325, 8016, 7881, 7823, 7699, 7656, 7656, 7463, 4908, 4166, 4157, 4151], 9: [8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8914, 8909, 8742, 7291, 7083, 2603], 10: [8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8916, 8916, 8915, 8906, 8904, 8884, 8868, 8868, 8868, 8867, 8867, 8867, 8867, 8866, 8864, 8862, 7629, 7510, 7344, 6654, 6471, 6077, 5507, 2326], 11: [8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8919, 8910, 7835, 5458, 2216], 12: [6042, 5729, 5623, 5417, 5369, 5305, 5190, 5123, 5080, 5080, 5013, 4988, 4955, 4945, 4933, 4878, 4813, 4799, 4709, 4708, 4689, 4688, 4671, 4654, 4619, 4594, 4583, 4579, 4563, 4562, 4538, 4461, 4461, 4446, 4440, 4439, 4408, 4353, 4353, 4351, 4336, 4320, 4317, 4315, 4308, 4287, 4239, 4228, 4208, 4198, 4189, 4131, 4112, 4095, 4092, 4081, 4049, 4047, 4036, 4031, 3962, 3958, 3926, 3924, 3905, 3893, 3854, 3850, 3847, 3821, 3798, 3797, 3797, 3789, 3783, 3741, 3739, 3701, 3699, 3693, 3683, 3680, 3666, 3663, 3637, 3633, 3614, 3597, 3585, 3584, 3579, 3570, 3567, 3561, 3544, 3510, 3499, 3497, 3492, 3484, 3447, 3417, 3385]}\n",
      "sensUnits: {0: [3, 21, 45, 32, 2, 0, 34, 4, 46, 35, 16, 58, 26], 1: [41, 58, 10, 7, 45, 50, 40, 51, 17, 0, 55, 43, 48], 2: [51, 18, 90, 120, 92, 17, 26, 32, 40, 13, 89, 72, 111, 109, 23, 74, 21, 105, 124, 122, 123, 10, 83, 47, 79, 4], 3: [6, 18, 49, 81, 74, 60, 103, 87, 32, 58, 110, 29, 79, 93, 105, 65, 78, 59, 26, 36, 10, 66, 42, 23, 9, 56], 4: [198, 35, 241, 125, 92, 126, 16, 216, 124, 36, 78, 130, 135, 31, 235, 74, 98, 236, 66, 65, 56, 21, 129, 175, 69, 6, 166, 82, 213, 242, 190, 95, 54, 144, 46, 253, 44, 221, 63, 72, 174, 33, 121, 186, 94, 178, 87, 239, 119, 146, 37, 205], 5: [163, 231, 190, 155, 138, 236, 146, 100, 74, 250, 36, 80, 187, 240, 134, 232, 19, 23, 110, 172, 64, 20, 216, 59, 27, 16, 184, 254, 15, 206, 46, 212, 90, 40, 213, 71, 33, 218, 183, 211, 38, 9, 44, 248, 182, 173, 228, 241, 129, 196, 244, 204], 6: [57, 214, 41, 16, 176, 135, 79, 118, 9, 161, 192, 143, 19, 170, 131, 175, 10, 0, 128, 134, 72, 93, 36, 149, 43, 195, 107, 223, 247, 105, 114, 2, 35, 233, 110, 78, 255, 206, 27, 60, 169, 234, 59, 249, 80, 165, 111, 237, 154, 26, 148, 53], 7: [463, 39, 98, 80, 140, 282, 267, 289, 395, 105, 441, 356, 442, 479, 250, 33, 346, 313, 383, 257, 240, 251, 403, 19, 194, 249, 95, 51, 300, 466, 163, 86, 40, 113, 126, 219, 333, 287, 224, 77, 49, 376, 398, 209, 31, 402, 110, 406, 68, 193, 495, 255, 302, 381, 334, 8, 458, 6, 59, 420, 26, 469, 132, 393, 4, 435, 326, 18, 369, 340, 448, 54, 286, 457, 183, 157, 210, 418, 121, 227, 417, 244, 261, 65, 187, 236, 78, 42, 37, 208, 84, 47, 415, 400, 89, 158, 159, 201, 145, 144, 165, 147, 156], 8: [60, 26, 212, 496, 492, 231, 154, 94, 324, 223, 33, 85, 71, 444, 148, 505, 410, 224, 10, 273, 157, 24, 330, 250, 421, 232, 399, 237, 290, 213, 169, 379, 128, 400, 326, 209, 186, 190, 299, 36, 366, 165, 204, 289, 42, 170, 241, 375, 371, 285, 335, 244, 101, 256, 300, 388, 317, 429, 220, 126, 465, 178, 450, 196, 427, 336, 151, 44, 468, 426, 478, 152, 158, 153, 160, 155, 156, 161, 139, 138, 140, 235, 227, 137, 226, 136, 159, 162, 142, 228, 359, 143, 141, 363, 233, 229, 163, 230, 145, 149, 146, 147, 144], 9: [265, 52, 490, 247, 342, 303, 106, 377, 259, 440, 198, 498, 479, 436, 27, 217, 193, 359, 343, 464, 386, 283, 109, 507, 459, 118, 33, 136, 51, 17, 140, 380, 285, 30, 233, 307, 238, 32, 124, 458, 108, 413, 366, 135, 74, 338, 474, 488, 94, 44, 485, 442, 329, 453, 137, 180, 309, 492, 258, 48, 308, 294, 128, 445, 480, 96, 155, 147, 154, 153, 146, 152, 163, 157, 161, 160, 145, 156, 143, 148, 149, 139, 168, 138, 172, 171, 144, 170, 151, 169, 162, 167, 166, 165, 164, 150, 158, 141, 279, 142, 411, 423, 173], 10: [184, 428, 202, 264, 312, 66, 138, 364, 504, 257, 133, 272, 232, 119, 124, 381, 342, 161, 359, 127, 165, 224, 308, 25, 469, 448, 293, 336, 349, 7, 319, 418, 234, 473, 340, 211, 146, 356, 305, 317, 40, 32, 63, 1, 299, 240, 352, 404, 360, 177, 170, 179, 92, 253, 250, 323, 368, 116, 175, 159, 160, 154, 155, 156, 157, 158, 152, 150, 153, 151, 143, 141, 142, 139, 149, 144, 147, 148, 140, 162, 145, 235, 163, 164, 236, 231, 230, 229, 227, 226, 233, 228, 225, 237, 407, 166, 223, 204, 183, 206, 192, 205, 207], 11: [400, 476, 287, 281, 181, 14, 440, 437, 411, 21, 69, 80, 317, 214, 306, 387, 430, 212, 379, 457, 35, 347, 483, 498, 40, 27, 276, 144, 141, 257, 424, 16, 48, 471, 31, 419, 246, 469, 435, 147, 322, 382, 466, 356, 458, 272, 240, 468, 138, 72, 341, 451, 494, 114, 398, 90, 313, 285, 477, 107, 5, 0, 267, 133, 456, 45, 71, 511, 474, 499, 168, 359, 129, 331, 126, 293, 218, 343, 110, 434, 70, 26, 386, 350, 335, 93, 448, 464, 491, 510, 30, 332, 413, 432, 325, 289, 416, 17, 4, 153, 393, 368, 369], 12: [286, 79, 164, 203, 131, 480, 25, 394, 209, 344, 127, 392, 34, 485, 362, 371, 473, 229, 126, 179, 273, 410, 125, 138, 175, 383, 350, 419, 85, 454, 257, 145, 67, 355, 364, 190, 504, 5, 152, 460, 327, 493, 478, 309, 33, 263, 291, 361, 340, 287, 490, 487, 124, 59, 269, 421, 61, 106, 45, 221, 440, 409, 445, 411, 307, 368, 385, 26, 468, 348, 353, 2, 274, 442, 317, 154, 278, 483, 84, 161, 509, 465, 502, 191, 417, 462, 261, 444, 187, 0, 201, 377, 219, 101, 407, 64, 208, 477, 252, 283, 107, 60, 237]}\n"
     ]
    }
   ],
   "source": [
    "model = VGG16(num_classes=10).cuda()\n",
    "model_path = \"/media/trs1/litl/Vanilla_VGG_16_copy.pkl\"\n",
    "# model_path = \"/media/trs1/TheVirtuoso/Model_Sensitivity/BN_VGG/PAT_VGG_16_finetune.pkl\"\n",
    "checkpoint = torch.load(model_path)\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()\n",
    "\n",
    "mask_model = mask_VGG16(10).cuda()\n",
    "model_path = \"/media/trs1/litl/Vanilla_VGG_16_copy.pkl\"\n",
    "checkpoint = torch.load(model_path)\n",
    "mask_model.load_state_dict(checkpoint)\n",
    "mask_model.eval()\n",
    "\n",
    "unit_num = 20\n",
    "unit_pick_path = 20\n",
    "\n",
    "\n",
    "transform_test = T.Compose([\n",
    "            T.ToTensor()\n",
    "])\n",
    "\n",
    "dataset = datasets.CIFAR10(\n",
    "        root = \"/media/trs1/yuhang/cifar-10\",\n",
    "        train = False,\n",
    "        transform = transform_test,\n",
    "        download = False\n",
    ")\n",
    "dataloader = Data.DataLoader(dataset=dataset, batch_size=64, shuffle=False, num_workers=2, drop_last=False)\n",
    "\n",
    "# root = \"/media/trs1/litl/Vanilla_key_paths_loss_lossOriented_abs/\"\n",
    "root = \"/media/trs1/litl/path_cifar/Vanilla_key_paths_loss_layer_abs/\"\n",
    "attack = \"clean\"\n",
    "data_path = root + \"key_paths_\" + attack + \"/key_paths_unit_per20.pkl\"\n",
    "# c_links_clean = get_criticalLinks(data_path, link_num)\n",
    "units_clean = get_sensUnit2(data_path, unit_num)\n",
    "\n",
    "transform_test = T.Compose([\n",
    "            T.ToTensor()\n",
    "])\n",
    "\n",
    "dataset = datasets.CIFAR10(\n",
    "        root = \"/media/trs1/yuhang/cifar-10\",\n",
    "        train = False,\n",
    "        transform = transform_test,\n",
    "        download = False\n",
    ")\n",
    "\n",
    "mean_features_specialUnits = {}\n",
    "mean_features_regularUnits = {}\n",
    "num_special = {}\n",
    "num_regular = {}\n",
    "for k in units_clean.keys():\n",
    "    conv_name = \"feat_conv{}_relu\".format(k+1)\n",
    "    mean_features_specialUnits[conv_name] = 0\n",
    "    mean_features_regularUnits[conv_name] = 0\n",
    "    num_special[conv_name] = 0\n",
    "    num_regular[conv_name] = 0\n",
    "\n",
    "data_root = '/media/trs1/yuhang/CIFAR-10-C/impulse_noise.npy'\n",
    "label_root = '/media/trs1/yuhang/CIFAR-10-C/labels.npy'\n",
    "\n",
    "def get_adv_dataset(x,y,test_batch_size):\n",
    "    test_data = torch.from_numpy(x).float()\n",
    "    test_label = torch.from_numpy(y).long()\n",
    "    adv_dataset = torch.utils.data.TensorDataset(test_data,test_label)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset = adv_dataset,\n",
    "                                              batch_size = test_batch_size,\n",
    "                                              shuffle = False,\n",
    "                                              drop_last = True)\n",
    "    return adv_dataset\n",
    "\n",
    "#load data\n",
    "x = np.load(data_root)\n",
    "x = x.transpose((0,3,1,2))\n",
    "x = x/255.0\n",
    "y = np.load(label_root)\n",
    "#data_loader\n",
    "adv_dataset = get_adv_dataset(x, y, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: [54, 51, 23, 5, 24, 47, 61, 25, 40, 31, 13, 60, 50], 1: [23, 24, 35, 32, 4, 33, 52, 60, 3, 36, 8, 44, 21], 2: [101, 81, 125, 104, 37, 20, 116, 54, 96, 76, 118, 45, 102, 28, 56, 86, 60, 73, 61, 34, 59, 93, 77, 12, 53, 9], 3: [2, 119, 83, 47, 89, 54, 86, 73, 31, 19, 104, 15, 76, 57, 72, 107, 8, 70, 115, 63, 77, 27, 99, 75, 22, 111], 4: [252, 215, 225, 237, 169, 112, 164, 77, 161, 139, 11, 86, 185, 170, 246, 145, 247, 99, 217, 233, 41, 107, 140, 38, 90, 50, 101, 0, 207, 142, 202, 243, 79, 191, 148, 136, 201, 168, 22, 29, 231, 106, 68, 158, 189, 234, 91, 143, 192, 105, 165, 73], 5: [147, 207, 140, 28, 73, 93, 208, 222, 3, 66, 7, 131, 4, 85, 96, 164, 176, 119, 34, 52, 127, 81, 153, 61, 5, 170, 106, 225, 175, 14, 77, 21, 149, 55, 70, 192, 194, 144, 137, 29, 98, 188, 111, 41, 35, 174, 97, 114, 247, 223, 47, 251], 6: [124, 14, 204, 173, 201, 116, 122, 88, 199, 178, 225, 184, 180, 153, 101, 94, 235, 209, 49, 213, 241, 73, 189, 64, 47, 240, 54, 113, 205, 210, 179, 208, 70, 39, 251, 252, 58, 244, 138, 115, 92, 23, 139, 120, 203, 145, 127, 22, 119, 196, 21, 65], 7: [380, 303, 129, 115, 462, 484, 468, 359, 401, 50, 234, 185, 136, 67, 506, 216, 9, 135, 164, 447, 1, 426, 56, 29, 382, 170, 335, 241, 370, 298, 259, 411, 507, 476, 229, 52, 392, 97, 503, 217, 274, 167, 21, 430, 169, 494, 205, 475, 436, 492, 429, 345, 262, 161, 351, 270, 277, 180, 471, 353, 207, 177, 24, 179, 196, 127, 71, 263, 349, 107, 308, 490, 148, 171, 343, 100, 299, 70, 344, 264, 168, 444, 385, 122, 73, 342, 511, 307, 508, 483, 160, 482, 318, 13, 433, 348, 424, 195, 223, 347, 45, 82, 371], 8: [219, 180, 127, 176, 407, 284, 27, 106, 130, 246, 486, 261, 13, 50, 96, 89, 84, 308, 419, 423, 460, 495, 239, 320, 339, 405, 41, 118, 221, 88, 15, 39, 62, 38, 208, 367, 198, 19, 115, 113, 346, 1, 322, 55, 255, 64, 504, 329, 214, 174, 75, 473, 306, 316, 0, 321, 67, 378, 457, 210, 283, 102, 63, 376, 77, 413, 202, 107, 318, 287, 411, 57, 11, 475, 438, 417, 80, 383, 447, 200, 451, 386, 72, 297, 422, 222, 112, 172, 476, 21, 234, 364, 509, 502, 389, 191, 9, 76, 466, 377, 372, 369, 464], 9: [465, 13, 39, 255, 209, 426, 427, 196, 368, 323, 315, 422, 90, 402, 46, 361, 73, 456, 225, 486, 197, 437, 103, 439, 333, 367, 230, 501, 298, 61, 435, 433, 273, 334, 510, 111, 210, 177, 178, 470, 428, 313, 425, 467, 86, 112, 277, 175, 318, 115, 509, 396, 5, 43, 104, 332, 250, 314, 31, 293, 190, 272, 99, 47, 270, 239, 328, 360, 344, 335, 345, 504, 481, 18, 288, 503, 352, 408, 468, 347, 187, 231, 321, 381, 373, 325, 452, 391, 127, 66, 116, 311, 11, 57, 60, 356, 117, 305, 281, 23, 397, 211, 6], 10: [49, 248, 15, 382, 444, 432, 11, 411, 12, 196, 399, 369, 220, 125, 328, 135, 397, 5, 26, 481, 421, 476, 23, 210, 9, 287, 22, 288, 500, 208, 396, 320, 55, 82, 251, 391, 430, 378, 72, 98, 331, 354, 384, 47, 442, 37, 19, 43, 195, 335, 276, 75, 470, 306, 113, 247, 425, 267, 67, 484, 33, 238, 283, 493, 6, 355, 126, 120, 387, 58, 462, 315, 408, 260, 460, 115, 467, 185, 309, 64, 471, 377, 62, 194, 245, 279, 14, 52, 0, 180, 69, 189, 380, 27, 494, 495, 274, 83, 499, 472, 285, 353, 435], 11: [490, 230, 220, 73, 391, 493, 509, 351, 290, 481, 159, 22, 160, 25, 124, 224, 171, 507, 7, 479, 496, 465, 56, 467, 241, 345, 3, 65, 75, 122, 88, 179, 228, 201, 390, 294, 205, 108, 450, 217, 79, 39, 128, 284, 91, 140, 310, 55, 199, 377, 383, 291, 223, 64, 503, 84, 207, 8, 183, 222, 348, 173, 24, 134, 143, 403, 169, 262, 488, 500, 161, 482, 278, 298, 409, 239, 401, 116, 34, 162, 362, 197, 117, 101, 83, 28, 259, 59, 349, 235, 213, 425, 439, 184, 307, 364, 319, 280, 296, 118, 263, 62, 113], 12: [116, 403, 399, 233, 358, 424, 96, 335, 57, 9, 230, 299, 375, 418, 319, 390, 446, 144, 474, 149, 378, 489, 234, 192, 215, 172, 53, 397, 357, 369, 370, 400, 38, 354, 416, 39, 58, 193, 270, 455, 382, 305, 249, 422, 304, 46, 185, 129, 279, 380, 7, 70, 280, 54, 181, 246, 165, 139, 236, 408, 231, 505, 449, 14, 298, 8, 359, 211, 301, 62, 423, 244, 81, 339, 324, 365, 113, 156, 167, 108, 282, 435, 420, 132, 496, 141, 155, 452, 471, 16, 475, 458, 123, 331, 295, 22, 235, 328, 98, 163, 296, 336, 346]}\n",
      "{0: [3, 21, 45, 32, 2, 0, 34, 4, 46, 35, 16, 58, 26], 1: [41, 58, 10, 7, 45, 50, 40, 51, 17, 0, 55, 43, 48], 2: [51, 18, 90, 120, 92, 17, 26, 32, 40, 13, 89, 72, 111, 109, 23, 74, 21, 105, 124, 122, 123, 10, 83, 47, 79, 4], 3: [6, 18, 49, 81, 74, 60, 103, 87, 32, 58, 110, 29, 79, 93, 105, 65, 78, 59, 26, 36, 10, 66, 42, 23, 9, 56], 4: [198, 35, 241, 125, 92, 126, 16, 216, 124, 36, 78, 130, 135, 31, 235, 74, 98, 236, 66, 65, 56, 21, 129, 175, 69, 6, 166, 82, 213, 242, 190, 95, 54, 144, 46, 253, 44, 221, 63, 72, 174, 33, 121, 186, 94, 178, 87, 239, 119, 146, 37, 205], 5: [163, 231, 190, 155, 138, 236, 146, 100, 74, 250, 36, 80, 187, 240, 134, 232, 19, 23, 110, 172, 64, 20, 216, 59, 27, 16, 184, 254, 15, 206, 46, 212, 90, 40, 213, 71, 33, 218, 183, 211, 38, 9, 44, 248, 182, 173, 228, 241, 129, 196, 244, 204], 6: [57, 214, 41, 16, 176, 135, 79, 118, 9, 161, 192, 143, 19, 170, 131, 175, 10, 0, 128, 134, 72, 93, 36, 149, 43, 195, 107, 223, 247, 105, 114, 2, 35, 233, 110, 78, 255, 206, 27, 60, 169, 234, 59, 249, 80, 165, 111, 237, 154, 26, 148, 53], 7: [463, 39, 98, 80, 140, 282, 267, 289, 395, 105, 441, 356, 442, 479, 250, 33, 346, 313, 383, 257, 240, 251, 403, 19, 194, 249, 95, 51, 300, 466, 163, 86, 40, 113, 126, 219, 333, 287, 224, 77, 49, 376, 398, 209, 31, 402, 110, 406, 68, 193, 495, 255, 302, 381, 334, 8, 458, 6, 59, 420, 26, 469, 132, 393, 4, 435, 326, 18, 369, 340, 448, 54, 286, 457, 183, 157, 210, 418, 121, 227, 417, 244, 261, 65, 187, 236, 78, 42, 37, 208, 84, 47, 415, 400, 89, 158, 159, 201, 145, 144, 165, 147, 156], 8: [60, 26, 212, 496, 492, 231, 154, 94, 324, 223, 33, 85, 71, 444, 148, 505, 410, 224, 10, 273, 157, 24, 330, 250, 421, 232, 399, 237, 290, 213, 169, 379, 128, 400, 326, 209, 186, 190, 299, 36, 366, 165, 204, 289, 42, 170, 241, 375, 371, 285, 335, 244, 101, 256, 300, 388, 317, 429, 220, 126, 465, 178, 450, 196, 427, 336, 151, 44, 468, 426, 478, 152, 158, 153, 160, 155, 156, 161, 139, 138, 140, 235, 227, 137, 226, 136, 159, 162, 142, 228, 359, 143, 141, 363, 233, 229, 163, 230, 145, 149, 146, 147, 144], 9: [265, 52, 490, 247, 342, 303, 106, 377, 259, 440, 198, 498, 479, 436, 27, 217, 193, 359, 343, 464, 386, 283, 109, 507, 459, 118, 33, 136, 51, 17, 140, 380, 285, 30, 233, 307, 238, 32, 124, 458, 108, 413, 366, 135, 74, 338, 474, 488, 94, 44, 485, 442, 329, 453, 137, 180, 309, 492, 258, 48, 308, 294, 128, 445, 480, 96, 155, 147, 154, 153, 146, 152, 163, 157, 161, 160, 145, 156, 143, 148, 149, 139, 168, 138, 172, 171, 144, 170, 151, 169, 162, 167, 166, 165, 164, 150, 158, 141, 279, 142, 411, 423, 173], 10: [184, 428, 202, 264, 312, 66, 138, 364, 504, 257, 133, 272, 232, 119, 124, 381, 342, 161, 359, 127, 165, 224, 308, 25, 469, 448, 293, 336, 349, 7, 319, 418, 234, 473, 340, 211, 146, 356, 305, 317, 40, 32, 63, 1, 299, 240, 352, 404, 360, 177, 170, 179, 92, 253, 250, 323, 368, 116, 175, 159, 160, 154, 155, 156, 157, 158, 152, 150, 153, 151, 143, 141, 142, 139, 149, 144, 147, 148, 140, 162, 145, 235, 163, 164, 236, 231, 230, 229, 227, 226, 233, 228, 225, 237, 407, 166, 223, 204, 183, 206, 192, 205, 207], 11: [400, 476, 287, 281, 181, 14, 440, 437, 411, 21, 69, 80, 317, 214, 306, 387, 430, 212, 379, 457, 35, 347, 483, 498, 40, 27, 276, 144, 141, 257, 424, 16, 48, 471, 31, 419, 246, 469, 435, 147, 322, 382, 466, 356, 458, 272, 240, 468, 138, 72, 341, 451, 494, 114, 398, 90, 313, 285, 477, 107, 5, 0, 267, 133, 456, 45, 71, 511, 474, 499, 168, 359, 129, 331, 126, 293, 218, 343, 110, 434, 70, 26, 386, 350, 335, 93, 448, 464, 491, 510, 30, 332, 413, 432, 325, 289, 416, 17, 4, 153, 393, 368, 369], 12: [286, 79, 164, 203, 131, 480, 25, 394, 209, 344, 127, 392, 34, 485, 362, 371, 473, 229, 126, 179, 273, 410, 125, 138, 175, 383, 350, 419, 85, 454, 257, 145, 67, 355, 364, 190, 504, 5, 152, 460, 327, 493, 478, 309, 33, 263, 291, 361, 340, 287, 490, 487, 124, 59, 269, 421, 61, 106, 45, 221, 440, 409, 445, 411, 307, 368, 385, 26, 468, 348, 353, 2, 274, 442, 317, 154, 278, 483, 84, 161, 509, 465, 502, 191, 417, 462, 261, 444, 187, 0, 201, 377, 219, 101, 407, 64, 208, 477, 252, 283, 107, 60, 237]}\n",
      "step: 0\n",
      "step: 1\n",
      "step: 2\n",
      "step: 3\n",
      "step: 4\n",
      "step: 5\n",
      "step: 6\n",
      "step: 7\n",
      "step: 8\n",
      "step: 9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 10\n",
      "step: 11\n",
      "step: 12\n",
      "step: 13\n",
      "step: 14\n",
      "step: 15\n",
      "step: 16\n",
      "step: 17\n",
      "step: 18\n",
      "step: 19\n",
      "step: 20\n",
      "step: 21\n",
      "step: 22\n",
      "step: 23\n",
      "step: 24\n",
      "step: 25\n",
      "step: 26\n",
      "step: 27\n",
      "step: 28\n",
      "step: 29\n",
      "step: 30\n",
      "step: 31\n",
      "step: 32\n",
      "step: 33\n",
      "step: 34\n",
      "step: 35\n",
      "step: 36\n",
      "step: 37\n",
      "step: 38\n",
      "step: 39\n",
      "step: 40\n",
      "step: 41\n",
      "step: 42\n",
      "step: 43\n",
      "step: 44\n",
      "step: 45\n",
      "step: 46\n",
      "step: 47\n",
      "step: 48\n",
      "step: 49\n",
      "step: 50\n",
      "step: 51\n",
      "step: 52\n",
      "step: 53\n",
      "step: 54\n",
      "step: 55\n",
      "step: 56\n",
      "step: 57\n",
      "step: 58\n",
      "step: 59\n",
      "step: 60\n",
      "step: 61\n",
      "step: 62\n",
      "step: 63\n",
      "step: 64\n",
      "step: 65\n",
      "step: 66\n",
      "step: 67\n",
      "step: 68\n",
      "step: 69\n",
      "step: 70\n",
      "step: 71\n",
      "step: 72\n",
      "step: 73\n",
      "step: 74\n",
      "step: 75\n",
      "step: 76\n",
      "step: 77\n",
      "step: 78\n",
      "step: 79\n",
      "step: 80\n",
      "step: 81\n",
      "step: 82\n",
      "step: 83\n",
      "step: 84\n",
      "step: 85\n",
      "step: 86\n",
      "step: 87\n",
      "step: 88\n",
      "step: 89\n",
      "step: 90\n",
      "step: 91\n",
      "step: 92\n",
      "step: 93\n",
      "step: 94\n",
      "step: 95\n",
      "step: 96\n",
      "step: 97\n",
      "step: 98\n",
      "step: 99\n",
      "step: 100\n",
      "step: 101\n",
      "step: 102\n",
      "step: 103\n",
      "step: 104\n",
      "step: 105\n",
      "step: 106\n",
      "step: 107\n",
      "step: 108\n",
      "step: 109\n",
      "step: 110\n",
      "step: 111\n",
      "step: 112\n",
      "step: 113\n",
      "step: 114\n",
      "step: 115\n",
      "step: 116\n",
      "step: 117\n",
      "step: 118\n",
      "step: 119\n",
      "step: 120\n",
      "step: 121\n",
      "step: 122\n",
      "step: 123\n",
      "step: 124\n",
      "step: 125\n",
      "step: 126\n",
      "step: 127\n",
      "step: 128\n",
      "step: 129\n",
      "step: 130\n",
      "step: 131\n",
      "step: 132\n",
      "step: 133\n",
      "step: 134\n",
      "step: 135\n",
      "step: 136\n",
      "step: 137\n",
      "step: 138\n",
      "step: 139\n",
      "step: 140\n",
      "step: 141\n",
      "step: 142\n",
      "step: 143\n",
      "step: 144\n",
      "step: 145\n",
      "step: 146\n",
      "step: 147\n",
      "step: 148\n",
      "step: 149\n",
      "step: 150\n",
      "step: 151\n",
      "step: 152\n",
      "step: 153\n",
      "step: 154\n",
      "step: 155\n",
      "step: 156\n",
      "[7618, 8706, 8846, 8872, 8913]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "start_index = 0\n",
    "\n",
    "for step, (x, y) in enumerate(dataloader): \n",
    "    x = x.cuda()\n",
    "    target, clean_feature_dict = model(x)  \n",
    "    break\n",
    "\n",
    "\n",
    "# random_units = {}\n",
    "# for i in units_clean:\n",
    "#     layer = \"feat_conv\" + str(i+1) + \"_relu\"\n",
    "#     random_units[i] = []\n",
    "#     while len(random_units[i]) < len(units_clean[i]):\n",
    "#         r = random.randint(0, clean_feature_dict[layer].size(1)-1)\n",
    "#         if not r in units_clean[i] and not r in random_units[i]:\n",
    "#             random_units[i].append(r)\n",
    "        \n",
    "# units = random_units\n",
    "units = units_clean\n",
    "\n",
    "print(random_units)\n",
    "print(units_clean)\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    sample_num = 0\n",
    "    adv_right0 = 0\n",
    "    adv_right1 = 0\n",
    "    adv_right3 = 0\n",
    "    adv_right5 = 0\n",
    "    adv_right7 = 0\n",
    "    clean_right = 0\n",
    "    for step, (x, y) in enumerate(dataloader):    \n",
    "        print(\"step:\", step)\n",
    "        batch_size = x.size(0)\n",
    "        adv_x = adv_dataset[start_index: start_index+batch_size][0]\n",
    "        start_index += x.size(0)\n",
    "        x = x.cuda()\n",
    "        y = y.cuda()\n",
    "        adv_x = adv_x.cuda()\n",
    "\n",
    "        target, clean_feature_dict = model(x)\n",
    "        adv_target0, adv_feature_dict0 = mask_model(adv_x, 0)\n",
    "        adv_target1, adv_feature_dict1 = mask_model(adv_x, 1)\n",
    "        adv_target3, adv_feature_dict3 = mask_model(adv_x, 3)\n",
    "        adv_target5, adv_feature_dict5 = mask_model(adv_x, 5)\n",
    "        adv_target7, adv_feature_dict7 = mask_model(adv_x, 7)\n",
    "\n",
    "        target = torch.max(target.data, 1)[1].data\n",
    "        adv_target0 = torch.max(adv_target0.data, 1)[1].data\n",
    "        adv_target1 = torch.max(adv_target1.data, 1)[1].data\n",
    "        adv_target3 = torch.max(adv_target3.data, 1)[1].data\n",
    "        adv_target5 = torch.max(adv_target5.data, 1)[1].data\n",
    "        adv_target7 = torch.max(adv_target7.data, 1)[1].data\n",
    "\n",
    "        clean_right += (target == y).sum()\n",
    "        adv_right0 += (adv_target0 == y).sum()\n",
    "        adv_right1 += (adv_target1 == y).sum() \n",
    "        adv_right3 += (adv_target3 == y).sum() \n",
    "        adv_right5 += (adv_target5 == y).sum() \n",
    "        adv_right7 += (adv_target7 == y).sum() \n",
    "\n",
    "print([adv_right0.item(), adv_right1.item(), adv_right3.item(), adv_right5.item(), adv_right7.item()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
